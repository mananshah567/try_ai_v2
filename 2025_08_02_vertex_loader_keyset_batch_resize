import time
from gremlin_python.driver.client import Client
from gremlin_python.driver.protocol import GremlinServerError
from ipywidgets import IntProgress, HTML, VBox
from IPython.display import display
from gremlin_python.process.traversal import P


def load_existing_vertices(
    client: Client,
    throttler,
    batch_size: int = 1000,
    minimal_batch: int = 100,
    max_retries: int = 3,
    backoff_base: float = 1.0
) -> set:
    """
    Robustly load all existing vertices via continuation tokens, with dynamic batch resizing on timeouts,
    retry/backoff on rate limits, RU throttling, and a notebook progress bar.
    Returns a set of vertex IDs.
    """
    continuation = None
    inserted_vertex_ids = set()

    # Get total count for progress bar (if possible)
    try:
        total = client.submit(
            "g.V().count()", {}, request_options={"pageSize":1}
        ).all().result()[0]
        vbar = IntProgress(min=0, max=total, description="Vertices:")
    except Exception:
        total = None
        vbar = IntProgress(min=0, description="Vertices:")

    vlabel = HTML()
    display(VBox([vlabel, vbar]))

    start = time.time()
    loaded = 0
    vlabel.value = f"Loaded 0{'/' + str(total) if total else ''} vertices..."

    current_batch = batch_size

    while True:
        # Dynamic fetch with batch-size reduction on timeout
        fetch_attempts = 0
        while True:
            try:
                rs = client.submit(
                    "g.V().id()",
                    {},
                    request_options={"continuation": continuation, "pageSize": current_batch}
                )
                verts = rs.all().result()
                ru = float(rs.status_attributes.get('x-ms-total-request-charge', 10.0))
                throttler.throttle(ru)
                break
            except GremlinServerError as e:
                msg = str(e)
                if 'GraphTimeoutException' in msg or '429' in msg:
                    fetch_attempts += 1
                    if fetch_attempts >= max_retries:
                        if current_batch <= minimal_batch:
                            print("⚠️ Fetch aborted: batch_size minimal; unable to proceed.")
                            return inserted_vertex_ids
                        # reduce batch and reset
                        current_batch = max(minimal_batch, current_batch // 2)
                        print(f"⚡ Reducing batch_size to {current_batch} and retrying...")
                        fetch_attempts = 0
                        time.sleep(backoff_base)
                    else:
                        time.sleep(backoff_base * (2 ** (fetch_attempts - 1)))
                    continue
                else:
                    raise

        if not verts:
            break

        # Collect & update
        for vid in verts:
            inserted_vertex_ids.add(vid)
        loaded += len(verts)
        vbar.value = loaded
        elapsed = time.time() - start
        rate = loaded / elapsed if elapsed else 0
        eta = (total - loaded) / rate if (total and rate) else float('inf')
        vlabel.value = (
            f"Loaded {loaded}{'/' + str(total) if total else ''} vertices. "
            f"Elapsed {elapsed:.1f}s. ETA {eta:.1f}s"
        )

        continuation = rs.status_attributes.get('x-ms-continuation')
        if not continuation:
            break

    return inserted_vertex_ids


def load_existing_vertices_by_keyset(
    client: Client,
    throttler,
    batch_size: int = 1000,
    minimal_batch: int = 100,
    max_retries: int = 3,
    backoff_base: float = 1.0
) -> set:
    """
    Keyset-paging loader over 'graph_id' with single-traversal projection of cursor + properties.
    Implements dynamic batch resizing on timeouts/429s, exponential backoff, RU throttling, and progress/ETA.
    Returns set of composite keys: "id|label|entity_type|entity_category".
    """
    last_gid = None
    inserted = set()
    current_batch = batch_size

    # progress bar sizing
    try:
        total = client.submit("g.V().count()", {}, request_options={"pageSize": 1}) \
                      .all().result()[0]
        bar = IntProgress(min=0, max=total, description="Vertices:")
    except Exception as e:
        print(f"Could not retrieve total count: {e}")
        total = None
        bar = IntProgress(min=0, description="Vertices:")
    label = HTML(); display(VBox([label, bar]))

    loaded = 0
    start = time.time()
    label.value = f"Loaded 0{'/' + str(total) if total else ''} vertices..."

    # projection including the graph_id cursor
    proj = (
        ".project('graph_id','id','label','entity_type','entity_category')"
        ".by(values('graph_id'))"
        ".by(id())"
        ".by(label())"
        ".by(values('entity_type').fold().coalesce(unfold(),constant('')))"
        ".by(values('entity_category').fold().coalesce(unfold(),constant('')))"
    )

    while True:
        if last_gid is None:
            gremlin = f"g.V().order().by('graph_id').limit({current_batch})" + proj
        else:
            gremlin = (
                f"g.V().has('graph_id', P.gt('{last_gid}')).order().by('graph_id')"
                f".limit({current_batch})" + proj
            )

        # fetch with retry/backoff and dynamic resizing
        fetch_attempts = 0
        while True:
            try:
                rs = client.submit(
                    gremlin,
                    {},
                    request_options={"pageSize": current_batch}
                )
                batch = rs.all().result()
                ru = float(rs.status_attributes.get("x-ms-total-request-charge", 10.0))
                throttler.throttle(ru)
                break
            except GremlinServerError as e:
                msg = str(e)
                print(f"Fetch attempt {fetch_attempts+1} failed for [{gremlin}]: {msg}")
                if "429" in msg or "GraphTimeoutException" in msg:
                    fetch_attempts += 1
                    if fetch_attempts >= max_retries:
                        if current_batch <= minimal_batch:
                            print("⚠️ Fetch aborted: batch_size minimal; unable to proceed.")
                            return inserted
                        current_batch = max(minimal_batch, current_batch // 2)
                        print(f"⚡ Reducing batch_size to {current_batch} and retrying...")
                        fetch_attempts = 0
                        time.sleep(backoff_base)
                        continue
                    time.sleep(backoff_base * (2 ** (fetch_attempts - 1)))
                    continue
                else:
                    raise

        if not batch:
            break  # done

        for item in batch:
            try:
                key = '|'.join(_safe(item[k]) for k in ['id', 'label', 'entity_type', 'entity_category'])
                inserted.add(key)
            except Exception as e:
                print(f"Processing error for item {item}: {e}")

        # advance cursor using last returned graph_id
        last_gid = _safe(batch[-1]['graph_id'])
        loaded += len(batch)
        bar.value = loaded
        elapsed = time.time() - start
        rate = loaded / elapsed if elapsed else 0
        eta = (total - loaded) / rate if (total and rate) else float('inf')
        label.value = f"Loaded {loaded}{'/' + str(total) if total else ''} · ETA {eta:.1f}s"

    return inserted
